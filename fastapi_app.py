'''from fastapi import FastAPI
from pydantic import BaseModel
from transformers import TFAutoModelForSequenceClassification, AutoTokenizer
import tensorflow as tf

app = FastAPI()

# Paths
MODEL_PATH = "C:/Users/FBC/Desktop/projects/chatbot/ticket_classifier_model"
TOKENIZER_PATH = "C:/Users/FBC/Desktop/projects/chatbot/ticket_classifier_tokenizer"

# Load model + tokenizer
model = TFAutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)

# Root endpoint for testing
@app.get("/")
def read_root():
    return {"message": "FastAPI server is running!"}

# Request model
class Ticket(BaseModel):
    text: str

# Predict endpoint
@app.post("/predict")
def predict(ticket: Ticket):
    inputs = tokenizer(ticket.text, return_tensors="tf", truncation=True, padding=True)
    outputs = model(inputs)
    logits = outputs.logits
    predicted_class = tf.argmax(logits, axis=1).numpy()[0]
    return {"prediction": int(predicted_class)}'''

from fastapi import FastAPI
from pydantic import BaseModel
from transformers import TFAutoModelForSequenceClassification, AutoTokenizer
import tensorflow as tf

app = FastAPI()

# Paths
MODEL_PATH = "C:/Users/FBC/Desktop/projects/chatbot/ticket_classifier_model"
TOKENIZER_PATH = "C:/Users/FBC/Desktop/projects/chatbot/ticket_classifier_tokenizer"

# Load model + tokenizer
model = TFAutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)

# Define categories & auto-replies
categories = ["Billing", "Technical", "Shipping", "Complaint"]
auto_reply_dict = {
    "Billing": "Your billing issue has been forwarded to our finance team.",
    "Technical": "Our technical team will look into your issue.",
    "Shipping": "Your shipping issue is being processed.",
    "Complaint": "We have received your complaint and will get back soon."
}

# Root endpoint for testing
@app.get("/")
def read_root():
    return {"message": "FastAPI server is running!"}

# Request model
class Ticket(BaseModel):
    text: str

# Predict endpoint
@app.post("/predict")
def predict(ticket: Ticket):
    # Tokenize input
    inputs = tokenizer(ticket.text, return_tensors="tf", truncation=True, padding=True)
    
    # Model prediction
    outputs = model(inputs)
    logits = outputs.logits
    predicted_class = tf.argmax(logits, axis=1).numpy()[0]

    # Map class index to category
    category = categories[predicted_class]
    auto_reply = auto_reply_dict[category]

    # Return both category & auto-reply
    return {"category": category, "auto_reply": auto_reply} 




'''import os
from fastapi import FastAPI
from pydantic import BaseModel
from transformers import TFAutoModelForSequenceClassification, AutoTokenizer
import tensorflow as tf
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import requests

# --- FastAPI App ---
app = FastAPI()

# Paths
MODEL_PATH = "C:/Users/FBC/Desktop/projects/chatbot/ticket_classifier_model"
TOKENIZER_PATH = "C:/Users/FBC/Desktop/projects/chatbot/ticket_classifier_tokenizer"

# Load classifier model + tokenizer
model = TFAutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)

# Categories
categories = ["Billing", "Technical", "Shipping", "Complaint"]

# --- Vector DB Setup (FAISS) ---
embedder = SentenceTransformer('all-MiniLM-L6-v2')
embedding_dim = 384
index = faiss.IndexFlatL2(embedding_dim)
ticket_history = []

# --- Gemini API Config ---
GEMINI_API_KEY = ("AIzaSyDdm9szAN98T7sj_a4AaKihByZrHQakjpI")  # Set your API key in environment
GEMINI_MODEL = "gemini-2.5-flash"      # Update with the Gemini model you want

def generate_gemini_reply(prompt: str):
    headers = {
        "x-goog-api-key": GEMINI_API_KEY,
        "Content-Type": "application/json"
    }
    data = {
        "contents": [
            {
                "parts": [
                    {"text": prompt}
                ]
            }
        ],
        "temperature": 0.7,
        "maxOutputTokens": 200
    }
    try:
        response = requests.post(headers=headers, json=data, timeout=15)
        response.raise_for_status()
        resp_json = response.json()
        # Extract generated text
        if "candidates" in resp_json and len(resp_json["candidates"]) > 0:
            return resp_json["candidates"][0]["content"]["parts"][0]["text"]
        return "No reply generated by Gemini."
    except requests.exceptions.RequestException as e:
        return f"Error from Gemini API: {e}"

# Root endpoint
@app.get("/")
def read_root():
    return {"message": "FastAPI server is running!"}

# Ticket request model
class Ticket(BaseModel):
    text: str

# --- Predict endpoint ---
@app.post("/predict")
def predict(ticket: Ticket):
    # 1️⃣ Classifier Prediction
    inputs = tokenizer(ticket.text, return_tensors="tf", truncation=True, padding=True)
    outputs = model(inputs)
    logits = outputs.logits
    predicted_class = tf.argmax(logits, axis=1).numpy()[0]
    category = categories[predicted_class]

    # 2️⃣ Embed ticket for vector DB
    ticket_embedding = embedder.encode([ticket.text])

    # 3️⃣ MCP Layer: find similar tickets
    similar_context = ""
    if len(ticket_history) > 0:
        D, I = index.search(ticket_embedding, k=min(5, len(ticket_history)))
        similar_tickets = [ticket_history[i]["text"] for i in I[0]]
        similar_context = "\n".join(similar_tickets)

    # 4️⃣ Gemini LLM Auto-Reply (context-aware)
    llm_prompt = f"Ticket: {ticket.text}\nCategory: {category}\nSimilar tickets:\n{similar_context}\nGenerate a helpful reply:"
    auto_reply = generate_gemini_reply(llm_prompt)

    # 5️⃣ Save ticket in history + vector DB
    ticket_history.append({"text": ticket.text, "embedding": ticket_embedding[0], "category": category})
    index.add(np.array(ticket_embedding, dtype=np.float32))

    return {
        "category": category,
        "auto_reply": auto_reply,
        "similar_tickets": similar_context.split("\n") if similar_context else []
    }'''


















# app.py

''' from fastapi import FastAPI
from pydantic import BaseModel
from transformers import TFAutoModelForSequenceClassification, AutoTokenizer
import tensorflow as tf
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import requests
import os

# --- FastAPI App ---
app = FastAPI()

# Paths
MODEL_PATH = "C:/Users/FBC/Desktop/projects/chatbot/ticket_classifier_model"
TOKENIZER_PATH = "C:/Users/FBC/Desktop/projects/chatbot/ticket_classifier_tokenizer"

# Load classifier model + tokenizer
model = TFAutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)

# Categories
categories = ["Billing", "Technical", "Shipping", "Complaint"]

# --- Vector DB Setup (FAISS) ---
embedder = SentenceTransformer('all-MiniLM-L6-v2')
embedding_dim = 384
index = faiss.IndexFlatL2(embedding_dim)  # FAISS vector index
ticket_history = []  # Store past tickets: text + embedding + category

# --- Gemini API Config ---
GEMINI_API_KEY = "AIzaSyDdm9szAN98T7sj_a4AaKihByZrHQakjpI"  # <-- Replace with your API key

GEMINI_API_URL = "https://api.gemini.com/v1/generate"  # Example, change as per Gemini docs

def generate_gemini_reply(prompt: str):
    headers = {
        "Authorization": f"Bearer {GEMINI_API_KEY}",
        "Content-Type": "application/json"
    }
    data = {"prompt": prompt, "max_tokens": 200}
    response = requests.post(GEMINI_API_URL, headers=headers, json=data)
    if response.status_code == 200:
        return response.json().get("text", "Sorry, could not generate a reply.")
    else:
        return f"Error from Gemini API: {response.status_code}"

# Root endpoint
@app.get("/")
def read_root():
    return {"message": "FastAPI server is running!"}

# Ticket request model
class Ticket(BaseModel):
    text: str

# --- Predict endpoint ---
@app.post("/predict")
def predict(ticket: Ticket):
    # 1️⃣ Classifier Prediction
    inputs = tokenizer(ticket.text, return_tensors="tf", truncation=True, padding=True)
    outputs = model(inputs)
    logits = outputs.logits
    predicted_class = tf.argmax(logits, axis=1).numpy()[0]
    category = categories[predicted_class]

    # 2️⃣ Embed ticket for vector DB
    ticket_embedding = embedder.encode([ticket.text])
    
    # 3️⃣ MCP Layer: find similar tickets
    similar_context = ""
    if len(ticket_history) > 0:
        D, I = index.search(ticket_embedding, k=min(5, len(ticket_history)))
        similar_tickets = [ticket_history[i]["text"] for i in I[0]]
        similar_context = "\n".join(similar_tickets)
    
    # 4️⃣ Gemini LLM Auto-Reply (context-aware)
    llm_prompt = f"Ticket: {ticket.text}\nCategory: {category}\nSimilar tickets:\n{similar_context}\nGenerate a helpful reply:"
    auto_reply = generate_gemini_reply(llm_prompt)

    # 5️⃣ Save ticket in history + vector DB
    ticket_history.append({"text": ticket.text, "embedding": ticket_embedding[0], "category": category})
    index.add(np.array(ticket_embedding, dtype=np.float32))

    # Return prediction + smart reply
    return {"category": category, "auto_reply": auto_reply, "similar_tickets": similar_context.split("\n") if similar_context else []}



# Run the app: uvicorn app:app --reload'''





